# Anki OCR vocab agent

This is a an easy 'n quick tool to create anki vocab cards using agents (smolagent library) and a visual model (`qwen2.5-vl`).

---

# What you‚Äôll end up with

Folder layout:

```
anki-ocr-agent/
‚îú‚îÄ‚îÄ .venv/                 # Python virtual env (auto-created by you)
‚îú‚îÄ‚îÄ vocab_ocr_agent.py     # the unified pipeline script
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îî‚îÄ‚îÄ vocabulary_page.png  # put your scanned image(s) here
‚îî‚îÄ‚îÄ anki_cards.csv         # will be created after running the script
```

# Step-by-step

## 1) Create a project folder

```bash
mkdir -p ~/anki-ocr-agent/input
cd ~/anki-ocr-agent
```

## 2) (If needed) Install Ollama and pull the model

If you already have Ollama + `qwen2.5-vl` locally, skip to step 3.

```bash
# Install Ollama (official installer)
curl -fsSL https://ollama.com/install.sh | sh

# Start/enable the Ollama service (Fedora/most systemd distros)
sudo systemctl enable --now ollama

# Pull the visual model
ollama pull qwen2.5vl:latest

# Start the model
ollama serve
```

> Tip: Check it‚Äôs installed with `ollama list`.

## 3) Create and activate a Python virtual env

```bash
python3 -m venv .venv
source .venv/bin/activate
```

A virtual environment (venv) in Python does not isolate system daemons or background services ‚Äî it only affects which Python packages and binaries you use.

So in your case:

ollama (the server listening on localhost:11434) is a system-wide process, not something inside your venv.

When you install a Python package (e.g. ollama, smolagents, etc.) inside a venv, it doesn‚Äôt re-run ollama itself ‚Äî it just gives you Python bindings or client code that talks to the Ollama server.

As long as ollama is running on port 11434, your venv‚Äôs Python scripts will be able to connect to it.

üëâ In other words: the ollama server runs once per system (outside venv), and any Python environment can connect to it. You don‚Äôt need a separate Ollama per venv.


## 4) Install Python packages

```bash
pip install --upgrade pip
pip install smolagents pyyaml
pip install smolagents[litellm] pyyaml

```

## 5) Save the unified pipeline script

Create `vocab_ocr_agent.py` in `~/anki-ocr-agent` and paste:

```python
import yaml
import csv
import re
import os
from smolagents import Tool, CodeAgent, OllamaModel


# ========== 1. YAML ‚Üí Anki Conversion Tool ==========
class YamlToAnkiTool(Tool):
    name = "yaml_to_anki"
    description = "Convert YAML vocabulary entries into an Anki-ready CSV file."

    inputs = {
        "yaml_content": {
            "type": "string",
            "description": "The pure YAML generated by the model."
        }
    }
    outputs = {
        "anki_file": {
            "type": "string",
            "description": "Path to the generated anki CSV file."
        }
    }

    OUTPUT_FILE = "anki_cards.csv"

    def is_proper_name(self, word):
        return bool(re.match(r'^[A-Z]', word))

    def forward(self, yaml_content: str):
        # Strip markdown fences if model wraps YAML in ``` blocks
        yaml_content = yaml_content.strip()
        if yaml_content.startswith("```") and yaml_content.endswith("```"):
            yaml_content = yaml_content[3:-3].strip()

        try:
            data = yaml.safe_load(yaml_content)
        except yaml.YAMLError as e:
            raise RuntimeError(f"‚ùå YAML error: {e}")

        seen = set()
        cleaned_data = []

        for entry in data:
            word = entry.get("word", "").strip()
            back = entry.get("back", "").strip()
            tags = entry.get("tags", "").strip()

            if not self.is_proper_name(word):
                word = word.lower()

            if word.lower() in seen:
                continue
            seen.add(word.lower())

            cleaned_data.append([word, back, tags])

        with open(self.OUTPUT_FILE, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["Word", "Back", "Tags"])
            writer.writerows(cleaned_data)

        return {"anki_file": os.path.abspath(self.OUTPUT_FILE)}


# ========== 2. OCR Extraction Prompt ==========
OCR_PROMPT = """
I will upload an image containing vocabulary words with their meanings and example sentences.

Your tasks are:
1. For each vocabulary entry, extract:
   - word: the vocabulary word (lowercase unless it is a personal/proper name, in which case keep original capitalization). If the word is a verb, replace it with the infinitive form.
   - back: 
       * Start with the meaning.
       * If the word has a noun alternative add an explanation of the noun.
       * Then add 2‚Äì3 examples, all enclosed in a single bracket ("example sentence", "..."). 
         Examples should be realistic and varied in structure.
   - tags: grammatical category such as "noun", "verb", "adjective", "adverb", etc (could be many tags)

2. If any field is missing, try to infer or improve it:
   - If examples are missing or too few, create new ones (ensure at least 2‚Äì3 examples per word).
   - If the meaning can be improved for clarity, do so.
   - If the grammatical tag is unclear, deduce it from the word and context.
   - If the word is a verb, determine and include its infinitive in the back field as described.

3. Output the result as **pure YAML** with the following rules:
   - Use `-` (dash) for each list item, never `*` or any other symbol.
   - Indent all fields by two spaces under the dash.
   - Do not include Markdown formatting, backticks, or any text outside the YAML.
   - Ensure proper YAML syntax so it can be parsed without errors.

The format must be exactly:

- word: example_word
  back: meaning (infinitive if verb). (_"example one"_) (_"example two"_) (_"example three"_)
  tags: noun

And the output should be in between ```
"""


# ========== 3. Model Setup ==========
# Use Ollama locally
model = OllamaModel("qwen2.5-vl:7b")


# ========== 4. Build Agent ==========
agent = CodeAgent(
    tools=[YamlToAnkiTool()],
    model=model,
    max_steps=3,
    verbosity="high"
)


# ========== 5. Run Agent on an Image ==========
def process_vocab_image(image_path: str):
    query = f"{OCR_PROMPT}\nHere is the vocabulary image: {image_path}\nExtract and then pass YAML to yaml_to_anki."
    result = agent.run(query)
    print("‚úÖ Pipeline finished. Anki CSV saved at anki_cards.csv")
    return result


if __name__ == "__main__":
    vocab_image = "input/vocabulary_page.png"  # place your scan in ./input/
    process_vocab_image(vocab_image)
```



## 6) Put your vocabulary image in `./input/`

Example:

```bash
cp /path/to/your/scan.png input/vocabulary_page.png
```

## 7) Run the pipeline

From the project folder:

```bash
python vocab_ocr_agent.py
```

What happens:

1. The agent loads **qwen2.5-vl** via Ollama.
2. It performs OCR/understanding on your image using the YAML prompt you provided.
3. It converts the YAML into **`anki_cards.csv`**.

If everything‚Äôs fine, you‚Äôll see a success message and `anki_cards.csv` will appear in the current folder.

---

# Import into Anki (manual)

Open Anki ‚Üí File ‚Üí Import ‚Üí select `anki_cards.csv`.
Map fields as:

* Field 1 ‚Üí **Front** (word)
* Field 2 ‚Üí **Back** (your meaning + examples)
* Field 3 ‚Üí **Tags** (optional tag field)

---

# Common bumps & quick fixes

* \*\*‚ÄúYAML error: found character ‚Äò`‚Äô‚Ä¶‚Äù**  
  That means the model wrapped the YAML in triple backticks. Quick fix: strip them before parsing.
  In `YamlToAnkiTool.forward`, add this right before `yaml.safe\_load(yaml\_content)\`:

  ````python
  yaml_content = yaml_content.strip()
  if yaml_content.startswith("```") and yaml_content.endswith("```"):
      yaml_content = yaml_content[3:-3].strip()
  ````

* **CSV not created**
  Rerun. Also ensure the prompt asks the agent to ‚Äúuse the `yaml_to_anki` tool‚Äù.
  (The unified script I gave already nudges it to call the tool.)

* **Slow on old CPU**
  That‚Äôs normal for VLMs. Start with smaller/clean images, and keep just the vocab area if you can.

* **Different model tag**
  If you pulled a different tag, just change:

  ```python
  model = OllamaModel("qwen2.5-vl:7b")
  ```

  to whatever `ollama list` shows you.

---

# TL;DR command list

```bash
# 1) Setup
mkdir -p ~/anki-ocr-agent/input
cd ~/anki-ocr-agent
python3 -m venv .venv
source .venv/bin/activate
pip install smolagents pyyaml

# 2) (If needed) Ollama + model
curl -fsSL https://ollama.com/install.sh | sh
sudo systemctl enable --now ollama
ollama pull qwen2.5-vl:7b

# 3) Add your script and image
#   - save vocab_ocr_agent.py here
#   - put your image at ./input/vocabulary_page.png

# 4) Run
python vocab_ocr_agent.py

# 5) Import anki_cards.csv into Anki
```

If you want, I can also give you a **batch mode** version (process all images in `./input/`) or wire it to **AnkiConnect** so it imports automatically after generating the CSV.




---

## Auto watch implementation

Got it üëç You want your `vocab_ocr_agent.py` to **auto-watch a folder** for new images and process them as soon as they appear ‚Äî no need to manually re-run. This is basically like ‚Äúloop mode + auto-watch.‚Äù

Here‚Äôs a clean version with that added:

```python
import os
import time
import pytesseract
from PIL import Image
import ollama
import argparse

def process_image(image_path, output_file):
    """Extract text from image and process with Ollama."""
    try:
        print(f"[INFO] Processing: {image_path}")
        img = Image.open(image_path)
        extracted_text = pytesseract.image_to_string(img)

        # Skip if no text found
        if not extracted_text.strip():
            print(f"[WARN] No text found in {image_path}")
            return

        prompt = f"""
        I will upload an image containing vocabulary words with their meanings and example sentences.

        Your tasks are:
        1. For each vocabulary entry, extract:
           - word: the vocabulary word (lowercase unless it is a personal/proper name, in which case keep original capitalization)
           - meaning: the definition of the word
           - example: an example sentence using the word

        Text:
        {extracted_text}
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": prompt}])
        result = response["message"]["content"]

        with open(output_file, "a", encoding="utf-8") as f:
            f.write(f"\n\n# Processed from {os.path.basename(image_path)}\n")
            f.write(result.strip())

        print(f"[SUCCESS] Results written to {output_file}")
    except Exception as e:
        print(f"[ERROR] Failed to process {image_path}: {e}")

def watch_folder(input_dir, output_file, interval=5):
    """Continuously watch a folder for new images and process them."""
    print(f"[WATCH] Monitoring folder: {input_dir}")
    seen = set()

    while True:
        try:
            # Look for new images
            images = [f for f in os.listdir(input_dir) if f.lower().endswith((".png", ".jpg", ".jpeg"))]
            new_images = [f for f in images if f not in seen]

            for img_file in new_images:
                img_path = os.path.join(input_dir, img_file)
                process_image(img_path, output_file)
                seen.add(img_file)

            time.sleep(interval)

        except KeyboardInterrupt:
            print("\n[EXIT] Stopped watching.")
            break

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="OCR Vocabulary Agent with Auto-Watch Mode")
    parser.add_argument("--input", type=str, required=True, help="Input directory with images")
    parser.add_argument("--output", type=str, required=True, help="Output markdown file")
    parser.add_argument("--watch", action="store_true", help="Enable auto-watch mode")
    parser.add_argument("--interval", type=int, default=5, help="Polling interval in seconds")
    args = parser.parse_args()

    if args.watch:
        watch_folder(args.input, args.output, args.interval)
    else:
        for img_file in os.listdir(args.input):
            if img_file.lower().endswith((".png", ".jpg", ".jpeg")):
                img_path = os.path.join(args.input, img_file)
                process_image(img_path, args.output)
```

---

### üîß How to use

1. Put this script as `vocab_ocr_agent.py` in your working folder.

2. Run **normal (batch) mode**:

   ```bash
   python vocab_ocr_agent.py --input ./images --output vocab.md
   ```

   ‚Üí Processes all images in `./images` once.

3. Run in **auto-watch mode**:

   ```bash
   python vocab_ocr_agent.py --input ./images --output vocab.md --watch
   ```

   ‚Üí Keeps watching `./images`. Whenever you drop a new `.png/.jpg/.jpeg`, it auto-extracts and appends results to `vocab.md`.

---